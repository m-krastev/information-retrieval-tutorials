{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1 <a class=\"anchor\" id=\"top\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# TODO: Ensure that no additional library is imported in the notebook.\n",
    "# TODO: Only the standard library and the following libraries are allowed:\n",
    "# TODO: You can also use unlisted classes from these libraries or standard libraries (such as defaultdict, Counter, ...).\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 1: Term-based Matching <a class=\"anchor\" id=\"part1\"></a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "In the first part, we will learn the basics of IR from loading and preprocessing the material, to implementing some well known search algorithms, to evaluating the ranking performance of the implemented algorithms. We will be using the CACM dataset throughout the assignment. The CACM dataset is a collection of titles and abstracts from the journal CACM (Communication of the ACM).\n",
    "\n",
    "Table of contents:\n",
    "- [Section 1: Text Processing](#text_processing)\n",
    "- [Section 2: Indexing](#indexing)\n",
    "- [Section 3: Ranking](#ranking)\n",
    "- [Section 4: Evaluation](#evaluation)\n",
    "- [Section 5: Analysis](#analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Text Processing <a class=\"anchor\" id=\"text_processing\"></a>\n",
    "\n",
    "[Back to Part 1](#part1)\n",
    "\n",
    "In this section, we will load the dataset and learn how to clean up the data to make it usable for an IR system.\n",
    "First, go through the implementation of the following functions:\n",
    "- `read_cacm_docs`: Reads in the CACM documents.\n",
    "- `read_queries`: Reads in the CACM queries.\n",
    "- `load_stopwords`: Loads the stopwords.\n",
    "\n",
    "The points of this section are earned for the following implementations:\n",
    "- `tokenize`: Tokenizes the input text.\n",
    "- `stem_token`: Stems the given token.\n",
    "\n",
    "We are using the [CACM dataset](http://ir.dcs.gla.ac.uk/resources/test_collections/cacm/), which is a small, classic IR dataset, composed of a collection of titles and abstracts from the journal CACM. It comes with relevance judgements for queries, so we can evaluate our IR system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.1 Read the CACM documents\n",
    "\n",
    "The following cell downloads the dataset and unzips it to a local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils import download_dataset\n",
    "\n",
    "download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You can see a brief description of each file in the dataset by looking at the README file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in this directory with sizes:\n",
      "          0 Jun 19 21:01 README\n",
      "\n",
      "    2187734 Jun 19 20:55 cacm.all              text of documents\n",
      "        626 Jun 19 20:58 cite.info             key to citation info\n",
      "                                                (the X sections in cacm.all)\n",
      "       2668 Jun 19 20:55 common_words           stop words used by smart\n",
      "       2194 Jun 19 20:55 make_coll*             shell script to make collection\n",
      "       1557 Jun 19 20:55 make_coll_term*        ditto (both useless without\n",
      "                                                smart system)\n",
      "       9948 Jun 19 20:55 qrels.text             relation giving\n",
      "                                                    qid did 0 0\n",
      "                                                to indicate dument did is\n",
      "                                                relevant to query qid\n",
      "      13689 Jun 19 20:55 query.text             Original text of the query\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Read the README file\n",
    "with open (\"./datasets/README\",\"r\") as file:\n",
    "    readme = file.read()\n",
    "    print(readme)\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We are interested in 4 files:\n",
    "- `cacm.all` : Contains the text for all documents. Note that some documents do not have abstracts available\n",
    "- `query.text` : The text of all queries\n",
    "- `qrels.text` : The relevance judgements\n",
    "- `common_words` : A list of common words. This may be used as a collection of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".I 1\n",
      ".T\n",
      "Preliminary Report-International Algebraic Language\n",
      ".B\n",
      "CACM December, 1958\n",
      ".A\n",
      "Perlis, A. J.\n",
      "Samelson,K.\n",
      ".N\n",
      "CA581203 JB March 22, 1978  8:28 PM\n",
      ".X\n",
      "100\t5\t1\n",
      "123\t5\t1\n",
      "164\t5\t1\n",
      "1\t5\t1\n",
      "1\t5\t1\n",
      "1\t5\t1\n",
      "205\t5\t1\n",
      "210\t5\t1\n",
      "214\t5\t1\n",
      "1982\t5\t1\n",
      "398\t5\t1\n",
      "642\t5\t1\n",
      "669\t5\t1\n",
      "1\t6\t1\n",
      "1\t6\t1\n",
      "1\t6\t1\n",
      "1\t6\t1\n",
      "1\t6\t1\n",
      "1\t6\t1\n",
      "1\t6\t1\n",
      "1\t6\t1\n",
      "1\t6\t1\n",
      "1\t6\t1\n",
      "165\t6\t1\n",
      "196\t6\t1\n",
      "196\t6\t1\n",
      "1273\t6\t1\n",
      "1883\t6\t1\n",
      "324\t6\t1\n",
      "43\t6\t1\n",
      "53\t6\t1\n",
      "91\t6\t1\n",
      "410\t6\t1\n",
      "3184\t6\t1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### The first 45 lines of the CACM dataset forms the first record\n",
    "# We are interested only in 3 fields.\n",
    "# 1. the '.I' field, which is the document id\n",
    "# 2. the '.T' field (the title) and\n",
    "# 3. the '.W' field (the abstract, which may be absent)\n",
    "with open (\"./datasets/cacm.all\",\"r\") as file:\n",
    "    cacm_all = \"\".join(file.readlines()[:45])\n",
    "    print(cacm_all)\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following function reads the `cacm.all` file. Note that each document has a variable number of lines. The `.I` field denotes a new document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Function\n",
    "from modules.dataset import read_cacm_docs\n",
    "\n",
    "docs = read_cacm_docs()\n",
    "n_docs = len(docs)\n",
    "\n",
    "assert isinstance(docs, list)\n",
    "assert n_docs == 3204, \"There should be exactly 3204 documents\"\n",
    "\n",
    "unzipped_docs = list(zip(*docs))\n",
    "assert np.sum(np.array(list(map(int,unzipped_docs[0])))) == 5134410"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Read the CACM queries\n",
    "\n",
    "Next, let us read the queries. They are formatted similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".I 1\n",
      ".W\n",
      " What articles exist which deal with TSS (Time Sharing System), an\n",
      "operating system for IBM computers?\n",
      ".N\n",
      " 1. Richard Alexander, Comp Serv, Langmuir Lab (TSS)\n",
      " \n",
      ".I 2\n",
      ".W\n",
      " I am interested in articles written either by Prieve or Udo Pooch\n",
      ".A\n",
      "Prieve, B.\n",
      "Pooch, U.\n",
      ".N\n",
      " 2. Richard Alexander, Comp Serv, Langmuir Lab (author = Pooch or Prieve)\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### The first 15 lines of 'query.text' has 2 queries\n",
    "# We are interested only in 2 fields.\n",
    "# 1. the '.I' - the query id\n",
    "# 2. the '.W' - the query\n",
    "with open (\"./datasets/query.text\",\"r\") as file:\n",
    "    query_file = \"\".join(file.readlines()[:16])\n",
    "    print(query_file)\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following function reads the `query.text` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.dataset import read_queries\n",
    "\n",
    "##### Function check\n",
    "queries = read_queries()\n",
    "\n",
    "assert isinstance(queries, list)\n",
    "assert len(queries) == 64 and all([q[1] is not None for q in queries]), \"There should be exactly 64 queries\"\n",
    "\n",
    "unzipped_queries = list(zip(*queries))\n",
    "assert np.sum(np.array(list(map(int,unzipped_queries[0])))) == 2080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.3 Read the stop words\n",
    "\n",
    "We use the common words stored in `common_words`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "about\n",
      "above\n",
      "accordingly\n",
      "across\n",
      "after\n",
      "afterwards\n",
      "again\n",
      "against\n",
      "all\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Read the stop words file\n",
    "with open (\"./datasets/common_words\",\"r\") as file:\n",
    "    sw_file = \"\".join(file.readlines()[:10])\n",
    "    print(sw_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following function reads the `common_words` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.preprocessing import load_stopwords\n",
    "##### Function check\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "assert isinstance(stopwords, set)\n",
    "assert len(stopwords) == 428, \"There should be exactly 428 stop words\"\n",
    "\n",
    "assert np.sum(np.array(list(map(len,stopwords)))) == 2234\n",
    "#####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.4 Tokenization \n",
    "\n",
    "We can now write some basic text processing functions.\n",
    "A first step is to tokenize the text.\n",
    "\n",
    "**Note**: Use the  `WordPunctTokenizer` available in the `nltk` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from modules.preprocessing import tokenize\n",
    "\n",
    "# ToDo:\n",
    "# Implement the function 'tokenize'.\n",
    "\n",
    "##### Function check\n",
    "text = \"the quick brown fox jumps over the lazy dog\"\n",
    "tokens = tokenize(text)\n",
    "\n",
    "print(tokens)\n",
    "# output: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.5 Stemming\n",
    "\n",
    "Write a function to stem tokens.\n",
    "Again, you can use the nltk library for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.preprocessing import stem_token\n",
    "# ToDo:\n",
    "# Implement the function 'stem_token'.\n",
    "\n",
    "##### Function check\n",
    "\n",
    "assert stem_token('test') is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.6 Summary\n",
    "\n",
    "The following function 'process_text' puts it all together. Given an input string, this functions tokenizes and processes it according to the flags that you set.\n",
    "This function is already implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.preprocessing import process_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's create two sets of preprocessed documents.\n",
    "We can process the documents and queries according to these two configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this configuration:\n",
    "# Don't preprocess the text, except to tokenize\n",
    "config_1 = {\n",
    "  \"stem\": False,\n",
    "  \"remove_stopwords\" : False,\n",
    "  \"lowercase_text\": True\n",
    "}\n",
    "\n",
    "\n",
    "# In this configuration:\n",
    "# Preprocess the text, stem and remove stopwords\n",
    "config_2 = {\n",
    "  \"stem\": True,\n",
    "  \"remove_stopwords\" : True,\n",
    "  \"lowercase_text\": True,\n",
    "}\n",
    "\n",
    "####\n",
    "doc_repr_1 = []\n",
    "doc_repr_2 = []\n",
    "for (doc_id, document) in docs:\n",
    "    doc_repr_1.append((doc_id, process_text(document, stopwords, **config_1)))\n",
    "    doc_repr_2.append((doc_id, process_text(document, stopwords, **config_2)))\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Indexing <a class=\"anchor\" id=\"indexing\"></a>\n",
    "\n",
    "[Back to Part 1](#part1)\n",
    "\n",
    "\n",
    "\n",
    "A retrieval function usually takes in a query document pair, and scores a query against a document.  Our document set is quite small - just a few thousand documents. However, consider a web-scale dataset with a few million documents. In such a scenario, it would become infeasible to score every query and document pair. In such a case, we can build an inverted index. From Wikipedia:\n",
    "\n",
    "> ... , an inverted index (also referred to as a postings file or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, .... The purpose of an inverted index is to allow fast full-text searches, at a cost of increased processing when a document is added to the database. ...\n",
    "\n",
    "\n",
    "Consider a simple inverted index, which maps from word to document. This can improve the performance of a retrieval system significantly. In this assignment, we consider a *simple* inverted index, which maps a word to a set of documents. In practice, however, more complex indices might be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Term Frequency-index \n",
    "In this assignment, we will be using an index created in memory since our dataset is tiny. To get started, build a simple index that maps each `token` to a list of `(doc_id, count)` where `count` is the count of the `token` in `doc_id`.\n",
    "For consistency, build this index using a python dictionary.\n",
    "\n",
    "Now, implement a function to build an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo:\n",
    "# Implement the function 'build_tf_index'\n",
    "\n",
    "from modules.indexing import build_tf_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now we can build indexed documents and preprocess the queries based on the two configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Indexed documents based on the two configs\n",
    "\n",
    "# Create the 2 indices\n",
    "tf_index_1 = build_tf_index(doc_repr_1)\n",
    "tf_index_2 = build_tf_index(doc_repr_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Ranking <a class=\"anchor\" id=\"ranking\"></a>\n",
    "\n",
    "[Back to Part 1](#part1)\n",
    "\n",
    "Now that we have cleaned and processed our dataset, we can start building simple IR systems.\n",
    "\n",
    "For now, we consider *simple* IR systems, which involve computing scores from the tokens present in the document/query. More advanced methods are covered in later assignments.\n",
    "\n",
    "We will implement the following methods in this section:\n",
    "- [Section 3.1: TF-IDF](#tfidf) \n",
    "- [Section 3.2: Query Likelihood Model](#qlm) \n",
    "- [Section 3.3: BM25](#bm25) \n",
    "\n",
    "*All search functions should be able to handle multiple words queries.*\n",
    "\n",
    "**Scoring policy:**\n",
    "Your implementations in this section are scored based on the expected performance of your ranking functions.\n",
    "You will get a full mark if your implementation meets the expected performance (measured by some evaluation metric).\n",
    "Otherwise, you may get partial credit.\n",
    "For example, if your *TF-IDF* ranking function has 60% of expected performance, you will get 6 out of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check the output of your ranking functions, you can use the predefined function\n",
    "'print_results'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_by_id = dict(docs)\n",
    "from modules.utils import print_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.1: TF-IDF <a class=\"anchor\" id=\"tfidf\"></a>\n",
    "\n",
    "Before we implement the tf-idf scoring functions, let's first write a function to compute the document frequencies of all words.\n",
    "\n",
    "#### 3.1.1 Document frequency\n",
    "Compute the document frequencies of all tokens in the collection.\n",
    "Your code should return a dictionary with tokens as its keys and the number of documents containing the token as values.\n",
    "For consistency, the values should have `int` type.\n",
    "\n",
    "You can use the pre-defined class DocumentFrequencies and the function 'get_df' in the rest of the assignemnt to get\n",
    "document frequencies based on the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo:\n",
    "#  Implement the following function 'compute_df'! \n",
    "from modules.ranking import compute_df\n",
    "\n",
    "#### Compute df based on the two configs\n",
    "\n",
    "# get the document frequencies of each document\n",
    "df_1 = compute_df([d[1] for d in doc_repr_1])\n",
    "df_2 = compute_df([d[1] for d in doc_repr_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the implementation of ranking functions smoother, you can use the class Dataset in modules.dataset that\n",
    "includes helper functions to get information about documents and indexes. Through this class you can employ the\n",
    "following functions:\n",
    "\n",
    "    - get_df\n",
    "    - get_doc_lengths\n",
    "    - get_index\n",
    "    - preprocess_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.dataset import Dataset\n",
    "\n",
    "dh1 = Dataset(n_docs, docs_by_id, doc_repr_1, df_1, tf_index_1, stopwords, config_1)\n",
    "\n",
    "dh2 = Dataset(n_docs, docs_by_id, doc_repr_2, df_2, tf_index_2, stopwords, config_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "#### Function check\n",
    "\n",
    "print(df_1['computer'])\n",
    "print(df_2['computer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 TF-IDF search \n",
    "Next, implement a function that computes a tf-idf score, given a query.\n",
    "Use the following formulas for TF and IDF:\n",
    "\n",
    "$$ TF=\\log (1 + f_{d,t}) $$\n",
    "\n",
    "$$ IDF=\\log (\\frac{N}{n_t})$$\n",
    "\n",
    "where $f_{d,t}$ is the frequency of token $t$ in document $d$, $N$ is the number of total documents and $n_t$ is the number of documents containing token $t$.\n",
    "\n",
    "**Note:** your implementation will be auto-graded assuming you have used the above formulas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Implement the following functions\n",
    "from modules.ranking import tfidf_tf_score, tfidf_idf_score, tfidf_term_score, tfidf_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Results:\n",
      "Rank 0(1.3e+01): PEEKABIT, Computer Offspring of Punched\\nCard PEEK...\n",
      "Rank 1(9.8): Variable Length Tree Structures Having Minimum Ave...\n",
      "Rank 2(8.2): A Stochastic Approach to the Grammatical Coding of...\n",
      "Rank 3(8.1): Full Table Quadratic Searching for Scatter Storage...\n",
      "Rank 4(7.6): Use of Tree Structures for Processing Files\\nIn da...\n"
     ]
    }
   ],
   "source": [
    "#### Function check\n",
    "test_tfidf = tfidf_search(\"computer word search\", dh2)[:5]\n",
    "print(f\"TFIDF Results:\")\n",
    "print_results(test_tfidf, docs_by_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 3.2: Query Likelihood Model  <a class=\"anchor\" id=\"qlm\"></a>\n",
    "\n",
    "In this section, you will implement a simple query likelihood model.\n",
    "\n",
    "\n",
    "#### 3.2.1 Naive QL \n",
    "\n",
    "First, let us implement a naive version of a QL model, assuming a multinomial unigram language model (with a uniform prior over the documents).\n",
    "You should ignore any querm term that does not appear in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo:\n",
    "#  Implement the following functions\n",
    "from modules.ranking import naive_ql_document_scoring, naive_ql_document_ranking, naive_ql_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive QL Results:\n",
      "Rank 0(2.8): A Report Writer For COBOL...\n",
      "Rank 1(2.8): A CRT Report Generating System...\n",
      "Rank 2(2.6): Preliminary Report-International Algebraic Languag...\n",
      "Rank 3(2.6): Supplement to the ALGOL 60 Report...\n",
      "Rank 4(2.5): ALGOL Sub-Committee Report - Extensions...\n"
     ]
    }
   ],
   "source": [
    "#### Function check\n",
    "test_naiveql = naive_ql_search(\"report\", dh1)[:5]\n",
    "print(f\"Naive QL Results:\")\n",
    "print_results(test_naiveql, docs_by_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 3.2.2 QL\n",
    "Now, let's implement a QL model that handles the issues with the naive version. In particular, you will implement a QL model with Jelinek-Mercer Smoothing. That means an interpolated score is computed per word - one term is the same as the previous naive version, and the second term comes from a unigram language model. In addition, you should accumulate the scores by summing the **log** (smoothed) probability which leads to better numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo:\n",
    "# Implement the following functions\n",
    "from modules.ranking import ql_background_model, ql_document_scoring, ql_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0(1.7): An Information Algebra - Phase I Report-Language\\n...\n",
      "Rank 1(1.7): Rejuvenating Experimental Computer Science\\nThis r...\n",
      "Rank 2(0.99): ALGOL 60 Confidential\\nThe ALGOL 60 Report,* when ...\n",
      "Rank 3(0.59): Automatic Abstracting and Indexing Survey and Reco...\n",
      "Rank 4(0.59): A String Language for Symbol Manipulation Based on...\n"
     ]
    }
   ],
   "source": [
    "#### Function check\n",
    "test_ql_results = ql_search(\"report\", dh1)[:5]\n",
    "print_results(test_ql_results, docs_by_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 3.3: BM25 <a class=\"anchor\" id=\"bm25\"></a>\n",
    "\n",
    "In this section, we will implement the BM25 scoring function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: \n",
    "# Implement the following functions\n",
    "from modules.ranking import bm25_tf_score, bm25_idf_score, bm25_term_score, bm25_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0(6.7): A Report Writer For COBOL...\n",
      "Rank 1(6.7): A CRT Report Generating System...\n",
      "Rank 2(6.6): Preliminary Report-International Algebraic Languag...\n",
      "Rank 3(6.6): Supplement to the ALGOL 60 Report...\n",
      "Rank 4(6.5): ALGOL Sub-Committee Report - Extensions...\n"
     ]
    }
   ],
   "source": [
    "#### Function check\n",
    "test_bm25_results = bm25_search(\"report\", dh1)[:5]\n",
    "print_results(test_bm25_results, docs_by_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 3.4. Test Your Functions\n",
    "\n",
    "The widget below allows you to play with the search functions you've written so far. Use this to test your search functions and ensure that they work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Highlighter function\n",
    "# class for results\n",
    "ResultRow = namedtuple(\"ResultRow\", [\"doc_id\", \"snippet\", \"score\"])\n",
    "# doc_id -> doc\n",
    "docs_by_id = dict((d[0], d[1]) for d in docs)\n",
    "\n",
    "def highlight_text(document, query, tol=17):\n",
    "    import re\n",
    "    tokens = tokenize(query)\n",
    "    regex = \"|\".join(f\"(\\\\b{t}\\\\b)\" for t in tokens)\n",
    "    regex = re.compile(regex, flags=re.IGNORECASE)\n",
    "    output = \"\"\n",
    "    i = 0\n",
    "    for m in regex.finditer(document):\n",
    "        start_idx = max(0, m.start() - tol)\n",
    "        end_idx = min(len(document), m.end() + tol)\n",
    "        output += \"\".join([\"...\",\n",
    "                        document[start_idx:m.start()],\n",
    "                        \"<strong>\",\n",
    "                        document[m.start():m.end()],\n",
    "                        \"</strong>\",\n",
    "                        document[m.end():end_idx],\n",
    "                        \"...\"])\n",
    "    return output.replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "def make_results(query, search_fn, index_set):\n",
    "    results = []\n",
    "    for doc_id, score in search_fn(query, index_set):\n",
    "        highlight = highlight_text(docs_by_id[doc_id], query)\n",
    "        if len(highlight.strip()) == 0:\n",
    "            highlight = docs_by_id[doc_id]\n",
    "        results.append(ResultRow(doc_id, highlight, score))\n",
    "    return results\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResultRow(doc_id='401', snippet='...n of Storage for <strong>Arrays</strong> in ALGOL 60 ...', score=8.041621375489514),\n",
       " ResultRow(doc_id='2064', snippet='...s on Generalized <strong>Arrays</strong> with the Genie C......ensional storage <strong>arrays</strong> are  standard fe......multidimensional <strong>arrays</strong> to themselves be......multidimensional <strong>arrays</strong>.  This system wa......ds may describe  <strong>arrays</strong> containing more ...', score=7.963346047396022),\n",
       " ResultRow(doc_id='433', snippet='...xed-World-Length <strong>Arrays</strong> in Variable-Word...', score=7.585177844142087),\n",
       " ResultRow(doc_id='3008', snippet='...age Proximity in <strong>Arrays</strong> Programmers and ......oblem of storing <strong>arrays</strong> as various kinds......tary  proof that <strong>arrays</strong> cannot be stored......it is shown that <strong>arrays</strong> cannot be stored......age strategy for <strong>arrays</strong>....', score=7.583737623561907),\n",
       " ResultRow(doc_id='2289', snippet='...Cellular <strong>Arrays</strong> for the Solution......ered by cellular <strong>arrays</strong> is the improveme......wn that cellular <strong>arrays</strong> are inherently w......e, the adjacency <strong>matrix</strong> of a graph is ea...... an array;  each <strong>matrix</strong> element is store...... use of cellular <strong>arrays</strong> for the  solutio...', score=7.182065497371145),\n",
       " ResultRow(doc_id='3124', snippet='... defining nested <strong>arrays</strong> in APL is presen......s as homogeneous <strong>arrays</strong> of numbers and c......nesting level of <strong>arrays</strong> and new operator...', score=6.881062747687482),\n",
       " ResultRow(doc_id='661', snippet='...ulti-dimensional <strong>arrays</strong> are  described i...', score=6.544992728309225),\n",
       " ResultRow(doc_id='2283', snippet='..., and Triangular <strong>Arrays</strong> In this report t......, and triangular <strong>arrays</strong>.  The approach t......of the other two <strong>arrays</strong>....', score=6.438993876712584),\n",
       " ResultRow(doc_id='3143', snippet='...Reasoning About <strong>Arrays</strong> A variety of con...... reasoning about <strong>arrays</strong>.  The basic conc...', score=6.282796826548969),\n",
       " ResultRow(doc_id='2488', snippet='...s derivative, to <strong>arrays</strong> of planar data....', score=5.9083765349241935)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_results('Matrix Arrays', bm25_search, dh1)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Evaluation <a class=\"anchor\" id=\"evaluation\"></a>\n",
    "\n",
    "[Back to Part 1](#part1)\n",
    "\n",
    "In order to analyze the effectiveness of retrieval algorithms, we first have to learn how to evaluate such a system. In particular, we will work with offline evaluation metrics. These metrics are computed on a dataset with known relevance judgements.\n",
    "\n",
    "Implement the following evaluation metrics.\n",
    "\n",
    "1. Precision \n",
    "2. Recall \n",
    "3. Mean Average Precision \n",
    "4. Expected Reciprocal Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.1 Read relevance labels\n",
    "\n",
    "Let's take a look at the `qrels.text` file, which contains the ground truth relevance scores. The relevance labels for CACM are binary - either 0 or 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 1410  0 0\n",
      "01 1572  0 0\n",
      "01 1605  0 0\n",
      "01 2020  0 0\n",
      "01 2358  0 0\n",
      "02 2434  0 0\n",
      "02 2863  0 0\n",
      "02 3078  0 0\n",
      "03 1134  0 0\n",
      "03 1613  0 0\n",
      "03 1807  0 0\n",
      "03 1947  0 0\n",
      "03 2290  0 0\n",
      "03 2923  0 0\n",
      "04 1749  0 0\n",
      "04 1811  0 0\n",
      "04 2256  0 0\n",
      "04 2371  0 0\n",
      "04 2597  0 0\n",
      "04 2796  0 0\n",
      "04 2912  0 0\n",
      "04 3043  0 0\n",
      "04 3073  0 0\n",
      "04 3082  0 0\n",
      "04 3127  0 0\n",
      "04 3128  0 0\n",
      "05 0756  0 0\n",
      "05 1307  0 0\n",
      "05 1502  0 0\n",
      "05 2035 0 0\n",
      "05 2299  0 0\n",
      "05 2399  0 0\n",
      "05 2501 0 0\n",
      "05 2820 0 0\n",
      "06 1543  0 0\n",
      "06 2078  0 0\n",
      "06 2828  0 0\n",
      "07 1198  0 0\n",
      "07 1338  0 0\n",
      "07 1877  0 0\n",
      "07 1960  0 0\n",
      "07 2150  0 0\n",
      "07 2228  0 0\n",
      "07 2256  0 0\n",
      "07 2280  0 0\n",
      "07 2320  0 0\n",
      "07 2342  0 0\n",
      "07 2376  0 0\n",
      "07 2482  0 0\n",
      "07 2578  0 0\n",
      "07 2597  0 0\n",
      "07 2618  0 0\n",
      "07 2685  0 0\n",
      "07 2700  0 0\n",
      "07 2777  0 0\n",
      "07 2865  0 0\n",
      "07 2866  0 0\n",
      "07 2895  0 0\n",
      "07 2912  0 0\n",
      "07 2941  0 0\n",
      "07 3043  0 0\n",
      "07 3082  0 0\n",
      "07 3128  0 0\n",
      "07 3141  0 0\n",
      "07 3148  0 0\n",
      "08 2625  0 0\n",
      "08 2849 0 0\n",
      "08 3032 0 0\n",
      "09 2372  0 0\n",
      "09 2632 0 0\n",
      "09 2870  0 0\n",
      "09 2876 0 0\n",
      "09 3068  0 0\n",
      "09 3111  0 0\n",
      "09 3128 0 0\n",
      "09 3158  0 0\n",
      "09 3177 0 0\n",
      "10 0046  0 0\n",
      "10 0141  0 0\n",
      "10 0392  0 0\n",
      "10 0950  0 0\n",
      "10 1158  0 0\n",
      "10 1198  0 0\n",
      "10 1262  0 0\n",
      "10 1380  0 0\n",
      "10 1471  0 0\n",
      "10 1601  0 0\n",
      "10 1613  0 0\n",
      "10 1747  0 0\n",
      "10 1795  0 0\n",
      "10 1811  0 0\n",
      "10 2060  0 0\n",
      "10 2150  0 0\n",
      "10 2256  0 0\n",
      "10 2289  0 0\n",
      "10 2342  0 0\n",
      "10 2376  0 0\n",
      "10 2433  0 0\n",
      "10 2618  0 0\n",
      "10 2664  0 0\n",
      "10 2685  0 0\n",
      "10 2700  0 0\n",
      "10 2714  0 0\n",
      "10 2777  0 0\n",
      "10 2785  0 0\n",
      "10 2851  0 0\n",
      "10 2895  0 0\n",
      "10 2896  0 0\n",
      "10 2912  0 0\n",
      "10 3039  0 0\n",
      "10 3075  0 0\n",
      "10 3156  0 0\n",
      "11 1043  0 0\n",
      "11 1188  0 0\n",
      "11 1306  0 0\n",
      "11 1358  0 0\n",
      "11 1396  0 0\n",
      "11 1491  0 0\n",
      "11 1923  0 0\n",
      "11 2246  0 0\n",
      "11 2316  0 0\n",
      "11 2527  0 0\n",
      "11 2699  0 0\n",
      "11 2710  0 0\n",
      "11 2715  0 0\n",
      "11 2716  0 0\n",
      "11 2906  0 0\n",
      "11 2923  0 0\n",
      "11 2956  0 0\n",
      "11 3073  0 0\n",
      "11 3150  0 0\n",
      "12 1523  0 0\n",
      "12 2080  0 0\n",
      "12 2246  0 0\n",
      "12 2629  0 0\n",
      "12 3127  0 0\n",
      "13 0115  0 0\n",
      "13 1223  0 0\n",
      "13 1231  0 0\n",
      "13 1551  0 0\n",
      "13 1625  0 0\n",
      "13 1795  0 0\n",
      "13 1807  0 0\n",
      "13 1947  0 0\n",
      "13 2495  0 0\n",
      "13 2579  0 0\n",
      "13 2897  0 0\n",
      "14 0074  0 0\n",
      "14 0117  0 0\n",
      "14 0232  0 0\n",
      "14 0776 0 0\n",
      "14 0827  0 0\n",
      "14 0850 0 0\n",
      "14 0851 0 0\n",
      "14 0852  0 0\n",
      "14 0854  0 0\n",
      "14 0855 0 0\n",
      "14 0856  0 0\n",
      "14 0857 0 0\n",
      "14 0858 0 0\n",
      "14 0860 0 0\n",
      "14 0861 0 0\n",
      "14 0862 0 0\n",
      "14 0864  0 0\n",
      "14 0865 0 0\n",
      "14 0866  0 0\n",
      "14 1175 0 0\n",
      "14 1724  0 0\n",
      "14 1919  0 0\n",
      "14 1956 0 0\n",
      "14 1969 0 0\n",
      "14 1980 0 0\n",
      "14 1997 0 0\n",
      "14 2017  0 0\n",
      "14 2041 0 0\n",
      "14 2108  0 0\n",
      "14 2118  0 0\n",
      "14 2146 0 0\n",
      "14 2176 0 0\n",
      "14 2191  0 0\n",
      "14 2272  0 0\n",
      "14 2337  0 0\n",
      "14 2348  0 0\n",
      "14 2397  0 0\n",
      "14 2563 0 0\n",
      "14 2664 0 0\n",
      "14 2679 0 0\n",
      "14 2714 0 0\n",
      "14 2716 0 0\n",
      "14 3075  0 0\n",
      "14 3187 0 0\n",
      "15 1231 0 0\n",
      "15 1551 0 0\n",
      "15 1613 0 0\n",
      "15 1947  0 0\n",
      "15 2263  0 0\n",
      "15 2495 0 0\n",
      "15 2598  0 0\n",
      "15 2685  0 0\n",
      "15 2701  0 0\n",
      "15 2880  0 0\n",
      "16 1746 0 0\n",
      "16 1749 0 0\n",
      "16 1828 0 0\n",
      "16 1854 0 0\n",
      "16 1960 0 0\n",
      "16 2070  0 0\n",
      "16 2114  0 0\n",
      "16 2342 0 0\n",
      "16 2376  0 0\n",
      "16 2378 0 0\n",
      "16 2500 0 0\n",
      "16 2632 0 0\n",
      "16 2817 0 0\n",
      "16 2912 0 0\n",
      "16 3073 0 0\n",
      "16 3105 0 0\n",
      "16 3148  0 0\n",
      "17 0115  0 0\n",
      "17 0405  0 0\n",
      "17 1134  0 0\n",
      "17 1223  0 0\n",
      "17 1231  0 0\n",
      "17 1535  0 0\n",
      "17 1551  0 0\n",
      "17 1613  0 0\n",
      "17 1807  0 0\n",
      "17 1934  0 0\n",
      "17 1947  0 0\n",
      "17 2290  0 0\n",
      "17 2495  0 0\n",
      "17 2579  0 0\n",
      "17 2586  0 0\n",
      "17 2923  0 0\n",
      "18 1158  0 0\n",
      "18 1215  0 0\n",
      "18 1262  0 0\n",
      "18 1471  0 0\n",
      "18 1613  0 0\n",
      "18 1811  0 0\n",
      "18 2060  0 0\n",
      "18 2175  0 0\n",
      "18 2413  0 0\n",
      "18 2433  0 0\n",
      "18 2685  0 0\n",
      "19 0141 0 0\n",
      "19 0863 0 0\n",
      "19 0950 0 0\n",
      "19 1601 0 0\n",
      "19 2266 0 0\n",
      "19 2664 0 0\n",
      "19 2714 0 0\n",
      "19 2973 0 0\n",
      "19 3075 0 0\n",
      "19 3156 0 0\n",
      "19 3175 0 0\n",
      "20 1563 0 0\n",
      "20 2695 0 0\n",
      "20 2986 0 0\n",
      "21 1429  0 0\n",
      "21 1847  0 0\n",
      "21 2189  0 0\n",
      "21 2490  0 0\n",
      "21 2603  0 0\n",
      "21 2701  0 0\n",
      "21 2702  0 0\n",
      "21 2703  0 0\n",
      "21 2932  0 0\n",
      "21 3018  0 0\n",
      "21 3139  0 0\n",
      "22 2369 0 0\n",
      "22 2384  0 0\n",
      "22 2441  0 0\n",
      "22 2473  0 0\n",
      "22 2564  0 0\n",
      "22 2637  0 0\n",
      "22 2638  0 0\n",
      "22 2678 0 0\n",
      "22 2692 0 0\n",
      "22 2751  0 0\n",
      "22 2760  0 0\n",
      "22 2761  0 0\n",
      "22 2827  0 0\n",
      "22 2828 0 0\n",
      "22 2829 0 0\n",
      "22 3116 0 0\n",
      "22 3149 0 0\n",
      "23 2578  0 0\n",
      "23 2849  0 0\n",
      "23 3137  0 0\n",
      "23 3148  0 0\n",
      "24 0268  0 0\n",
      "24 1696  0 0\n",
      "24 1892  0 0\n",
      "24 2069  0 0\n",
      "24 2123  0 0\n",
      "24 2297  0 0\n",
      "24 2373  0 0\n",
      "24 2667  0 0\n",
      "24 2862  0 0\n",
      "24 2970  0 0\n",
      "24 2996  0 0\n",
      "24 3078  0 0\n",
      "24 3098  0 0\n",
      "25 0268  0 0\n",
      "25 0757  0 0\n",
      "25 0963  0 0\n",
      "25 1408  0 0\n",
      "25 1518  0 0\n",
      "25 1526  0 0\n",
      "25 1533  0 0\n",
      "25 1572  0 0\n",
      "25 1653  0 0\n",
      "25 1698  0 0\n",
      "25 1719  0 0\n",
      "25 1805  0 0\n",
      "25 1892  0 0\n",
      "25 1901  0 0\n",
      "25 2085  0 0\n",
      "25 2095  0 0\n",
      "25 2218  0 0\n",
      "25 2277  0 0\n",
      "25 2318  0 0\n",
      "25 2319  0 0\n",
      "25 2358  0 0\n",
      "25 2373  0 0\n",
      "25 2434  0 0\n",
      "25 2452  0 0\n",
      "25 2535  0 0\n",
      "25 2582  0 0\n",
      "25 2667  0 0\n",
      "25 2668  0 0\n",
      "25 2669  0 0\n",
      "25 2681  0 0\n",
      "25 2741  0 0\n",
      "25 2765  0 0\n",
      "25 2798  0 0\n",
      "25 2818  0 0\n",
      "25 2831  0 0\n",
      "25 2859  0 0\n",
      "25 2862  0 0\n",
      "25 2863  0 0\n",
      "25 2881  0 0\n",
      "25 2918  0 0\n",
      "25 2928  0 0\n",
      "25 2984  0 0\n",
      "25 2988  0 0\n",
      "25 2996  0 0\n",
      "25 3006  0 0\n",
      "25 3048  0 0\n",
      "25 3059  0 0\n",
      "25 3067  0 0\n",
      "25 3088  0 0\n",
      "25 3089  0 0\n",
      "25 3119  0 0\n",
      "26 1071  0 0\n",
      "26 1198  0 0\n",
      "26 1338  0 0\n",
      "26 1749  0 0\n",
      "26 1828  0 0\n",
      "26 1854  0 0\n",
      "26 1960  0 0\n",
      "26 2080  0 0\n",
      "26 2150  0 0\n",
      "26 2256  0 0\n",
      "26 2320  0 0\n",
      "26 2342  0 0\n",
      "26 2376  0 0\n",
      "26 2379  0 0\n",
      "26 2541  0 0\n",
      "26 2597  0 0\n",
      "26 2618  0 0\n",
      "26 2632  0 0\n",
      "26 2700  0 0\n",
      "26 2740  0 0\n",
      "26 2777  0 0\n",
      "26 2851  0 0\n",
      "26 2866  0 0\n",
      "26 2912  0 0\n",
      "26 2938  0 0\n",
      "26 3039  0 0\n",
      "26 3043  0 0\n",
      "26 3048  0 0\n",
      "26 3082  0 0\n",
      "26 3128  0 0\n",
      "27 1641  0 0\n",
      "27 1642  0 0\n",
      "27 1750  0 0\n",
      "27 1752  0 0\n",
      "27 1879  0 0\n",
      "27 1884  0 0\n",
      "27 1901  0 0\n",
      "27 2095  0 0\n",
      "27 2297  0 0\n",
      "27 2435  0 0\n",
      "27 2481  0 0\n",
      "27 2498  0 0\n",
      "27 2560  0 0\n",
      "27 2596  0 0\n",
      "27 2669  0 0\n",
      "27 2734  0 0\n",
      "27 2747  0 0\n",
      "27 2768  0 0\n",
      "27 2798  0 0\n",
      "27 2818  0 0\n",
      "27 2859  0 0\n",
      "27 2864  0 0\n",
      "27 2902  0 0\n",
      "27 2918  0 0\n",
      "27 2955  0 0\n",
      "27 2983  0 0\n",
      "27 2988  0 0\n",
      "27 3000  0 0\n",
      "27 3052  0 0\n",
      "28 2578 0 0\n",
      "28 2849  0 0\n",
      "28 2890  0 0\n",
      "28 2949  0 0\n",
      "28 3032  0 0\n",
      "29 0377  0 0\n",
      "29 0513  0 0\n",
      "29 0610  0 0\n",
      "29 0935  0 0\n",
      "29 1094  0 0\n",
      "29 1420  0 0\n",
      "29 1537  0 0\n",
      "29 1538  0 0\n",
      "29 1539  0 0\n",
      "29 1840  0 0\n",
      "29 1841  0 0\n",
      "29 1967  0 0\n",
      "29 2028  0 0\n",
      "29 2089  0 0\n",
      "29 2120  0 0\n",
      "29 2462  0 0\n",
      "29 2927  0 0\n",
      "29 2932  0 0\n",
      "29 3037  0 0\n",
      "30 1926  0 0\n",
      "30 2486  0 0\n",
      "30 2786  0 0\n",
      "30 2917  0 0\n",
      "31 2125  0 0\n",
      "31 3047  0 0\n",
      "32 0366  0 0\n",
      "32 1145  0 0\n",
      "32 3139  0 0\n",
      "33 2805  0 0\n",
      "36 1265  0 0\n",
      "36 1350  0 0\n",
      "36 1683  0 0\n",
      "36 1768  0 0\n",
      "36 1787  0 0\n",
      "36 1825  0 0\n",
      "36 1836  0 0\n",
      "36 2015  0 0\n",
      "36 2084  0 0\n",
      "36 2110  0 0\n",
      "36 2179  0 0\n",
      "36 2340  0 0\n",
      "36 2423  0 0\n",
      "36 2702  0 0\n",
      "36 2708  0 0\n",
      "36 2733  0 0\n",
      "36 2824  0 0\n",
      "36 2836  0 0\n",
      "36 2986  0 0\n",
      "36 3094  0 0\n",
      "37 2265 0 0\n",
      "37 2377 0 0\n",
      "37 2558 0 0\n",
      "37 2625 0 0\n",
      "37 2632 0 0\n",
      "37 2651 0 0\n",
      "37 2738 0 0\n",
      "37 2840 0 0\n",
      "37 2939 0 0\n",
      "37 2941 0 0\n",
      "37 3144 0 0\n",
      "37 3148 0 0\n",
      "38 2265 0 0\n",
      "38 2558 0 0\n",
      "38 2625 0 0\n",
      "38 2632 0 0\n",
      "38 2651 0 0\n",
      "38 2868 0 0\n",
      "38 2939 0 0\n",
      "38 2940 0 0\n",
      "38 2941 0 0\n",
      "38 2956 0 0\n",
      "38 2957 0 0\n",
      "38 2958 0 0\n",
      "38 2960 0 0\n",
      "38 3031 0 0\n",
      "38 3103 0 0\n",
      "38 3150 0 0\n",
      "39 1693 0 0\n",
      "39 1861 0 0\n",
      "39 2126 0 0\n",
      "39 2265 0 0\n",
      "39 2317 0 0\n",
      "39 2558 0 0\n",
      "39 2625 0 0\n",
      "39 2632 0 0\n",
      "39 2651 0 0\n",
      "39 2939 0 0\n",
      "39 2941 0 0\n",
      "39 3031 0 0\n",
      "40 1614 0 0\n",
      "40 2126 0 0\n",
      "40 2148 0 0\n",
      "40 2265 0 0\n",
      "40 2651 0 0\n",
      "40 2939 0 0\n",
      "40 2940 0 0\n",
      "40 2941 0 0\n",
      "40 2956 0 0\n",
      "40 2958 0 0\n",
      "42 0963 0 0\n",
      "42 1069 0 0\n",
      "42 1518 0 0\n",
      "42 1572 0 0\n",
      "42 1653 0 0\n",
      "42 1805 0 0\n",
      "42 1827 0 0\n",
      "42 1884 0 0\n",
      "42 2022 0 0\n",
      "42 2085 0 0\n",
      "42 2151 0 0\n",
      "42 2247 0 0\n",
      "42 2318 0 0\n",
      "42 2344 0 0\n",
      "42 2522 0 0\n",
      "42 2542 0 0\n",
      "42 2749 0 0\n",
      "42 2951 0 0\n",
      "42 2984 0 0\n",
      "42 3048 0 0\n",
      "42 3072 0 0\n",
      "43 0122 0 0\n",
      "43 0266 0 0\n",
      "43 0297 0 0\n",
      "43 0462 0 0\n",
      "43 1113 0 0\n",
      "43 1325 0 0\n",
      "43 1528 0 0\n",
      "43 1554 0 0\n",
      "43 1686 0 0\n",
      "43 1697 0 0\n",
      "43 2004 0 0\n",
      "43 2195 0 0\n",
      "43 2201 0 0\n",
      "43 2211 0 0\n",
      "43 2382 0 0\n",
      "43 2400 0 0\n",
      "43 2421 0 0\n",
      "43 2514 0 0\n",
      "43 2523 0 0\n",
      "43 2655 0 0\n",
      "43 2687 0 0\n",
      "43 2751 0 0\n",
      "43 2754 0 0\n",
      "43 2771 0 0\n",
      "43 2788 0 0\n",
      "43 2811 0 0\n",
      "43 2826 0 0\n",
      "43 2827 0 0\n",
      "43 2828 0 0\n",
      "43 2829 0 0\n",
      "43 2841 0 0\n",
      "43 2883 0 0\n",
      "43 2910 0 0\n",
      "43 2913 0 0\n",
      "43 2924 0 0\n",
      "43 2994 0 0\n",
      "43 3047 0 0\n",
      "43 3062 0 0\n",
      "43 3116 0 0\n",
      "43 3149 0 0\n",
      "43 3172 0 0\n",
      "44 1804 0 0\n",
      "44 1891 0 0\n",
      "44 2004 0 0\n",
      "44 2382 0 0\n",
      "44 2514 0 0\n",
      "44 2523 0 0\n",
      "44 2547 0 0\n",
      "44 2687 0 0\n",
      "44 2751 0 0\n",
      "44 2771 0 0\n",
      "44 2827 0 0\n",
      "44 2829 0 0\n",
      "44 2910 0 0\n",
      "44 2913 0 0\n",
      "44 2924 0 0\n",
      "44 3013 0 0\n",
      "44 3047 0 0\n",
      "45 0268 0 0\n",
      "45 1831 0 0\n",
      "45 1935 0 0\n",
      "45 2140 0 0\n",
      "45 2257 0 0\n",
      "45 2359 0 0\n",
      "45 2360 0 0\n",
      "45 2452 0 0\n",
      "45 2493 0 0\n",
      "45 2669 0 0\n",
      "45 2680 0 0\n",
      "45 2716 0 0\n",
      "45 2765 0 0\n",
      "45 2816 0 0\n",
      "45 2878 0 0\n",
      "45 2882 0 0\n",
      "45 2900 0 0\n",
      "45 2964 0 0\n",
      "45 2965 0 0\n",
      "45 2969 0 0\n",
      "45 3002 0 0\n",
      "45 3058 0 0\n",
      "45 3129 0 0\n",
      "45 3137 0 0\n",
      "45 3152 0 0\n",
      "45 3168 0 0\n",
      "48 0149 0 0\n",
      "48 1353 0 0\n",
      "48 1666 0 0\n",
      "48 1729 0 0\n",
      "48 1797 0 0\n",
      "48 1863 0 0\n",
      "48 2073 0 0\n",
      "48 2223 0 0\n",
      "48 2226 0 0\n",
      "48 2285 0 0\n",
      "48 2325 0 0\n",
      "48 2589 0 0\n",
      "49 1152 0 0\n",
      "49 1515 0 0\n",
      "49 1681 0 0\n",
      "49 2127 0 0\n",
      "49 2390 0 0\n",
      "49 2561 0 0\n",
      "49 2795 0 0\n",
      "49 2832 0 0\n",
      "57 3077 0 0\n",
      "58 0432 0 0\n",
      "58 0536 0 0\n",
      "58 1293 0 0\n",
      "58 1344 0 0\n",
      "58 1398 0 0\n",
      "58 1411 0 0\n",
      "58 1420 0 0\n",
      "58 1445 0 0\n",
      "58 1619 0 0\n",
      "58 1629 0 0\n",
      "58 1631 0 0\n",
      "58 1691 0 0\n",
      "58 1709 0 0\n",
      "58 1812 0 0\n",
      "58 1944 0 0\n",
      "58 2098 0 0\n",
      "58 2115 0 0\n",
      "58 2122 0 0\n",
      "58 2123 0 0\n",
      "58 2249 0 0\n",
      "58 2349 0 0\n",
      "58 2395 0 0\n",
      "58 2634 0 0\n",
      "58 2636 0 0\n",
      "58 2719 0 0\n",
      "58 2731 0 0\n",
      "58 2825 0 0\n",
      "58 3159 0 0\n",
      "58 3166 0 0\n",
      "58 3167 0 0\n",
      "59 0440 0 0\n",
      "59 0944 0 0\n",
      "59 1112 0 0\n",
      "59 1170 0 0\n",
      "59 1235 0 0\n",
      "59 1314 0 0\n",
      "59 1324 0 0\n",
      "59 1456 0 0\n",
      "59 1457 0 0\n",
      "59 1700 0 0\n",
      "59 1785 0 0\n",
      "59 1786 0 0\n",
      "59 1855 0 0\n",
      "59 1860 0 0\n",
      "59 1885 0 0\n",
      "59 1973 0 0\n",
      "59 2018 0 0\n",
      "59 2032 0 0\n",
      "59 2033 0 0\n",
      "59 2092 0 0\n",
      "59 2107 0 0\n",
      "59 2111 0 0\n",
      "59 2127 0 0\n",
      "59 2203 0 0\n",
      "59 2251 0 0\n",
      "59 2274 0 0\n",
      "59 2359 0 0\n",
      "59 2412 0 0\n",
      "59 2524 0 0\n",
      "59 2530 0 0\n",
      "59 2532 0 0\n",
      "59 2537 0 0\n",
      "59 2543 0 0\n",
      "59 2552 0 0\n",
      "59 2559 0 0\n",
      "59 2631 0 0\n",
      "59 2673 0 0\n",
      "59 2905 0 0\n",
      "59 2974 0 0\n",
      "59 2991 0 0\n",
      "59 3053 0 0\n",
      "59 3083 0 0\n",
      "59 3126 0 0\n",
      "60 2023 0 0\n",
      "60 2046 0 0\n",
      "60 2198 0 0\n",
      "60 2377 0 0\n",
      "60 2406 0 0\n",
      "60 2452 0 0\n",
      "60 2493 0 0\n",
      "60 2516 0 0\n",
      "60 2526 0 0\n",
      "60 2593 0 0\n",
      "60 2710 0 0\n",
      "60 2715 0 0\n",
      "60 2716 0 0\n",
      "60 2717 0 0\n",
      "60 2718 0 0\n",
      "60 2765 0 0\n",
      "60 2816 0 0\n",
      "60 2817 0 0\n",
      "60 2882 0 0\n",
      "60 2957 0 0\n",
      "60 2959 0 0\n",
      "60 2960 0 0\n",
      "60 2964 0 0\n",
      "60 2976 0 0\n",
      "60 3087 0 0\n",
      "60 3137 0 0\n",
      "60 3147 0 0\n",
      "61 0239 0 0\n",
      "61 0440 0 0\n",
      "61 0634 0 0\n",
      "61 1032 0 0\n",
      "61 1236 0 0\n",
      "61 1457 0 0\n",
      "61 1514 0 0\n",
      "61 1675 0 0\n",
      "61 1830 0 0\n",
      "61 1927 0 0\n",
      "61 1976 0 0\n",
      "61 2160 0 0\n",
      "61 2307 0 0\n",
      "61 2363 0 0\n",
      "61 2451 0 0\n",
      "61 2452 0 0\n",
      "61 2524 0 0\n",
      "61 2575 0 0\n",
      "61 2631 0 0\n",
      "61 2641 0 0\n",
      "61 2711 0 0\n",
      "61 2765 0 0\n",
      "61 2965 0 0\n",
      "61 2966 0 0\n",
      "61 2976 0 0\n",
      "61 2990 0 0\n",
      "61 3001 0 0\n",
      "61 3012 0 0\n",
      "61 3134 0 0\n",
      "61 3168 0 0\n",
      "61 3169 0 0\n",
      "62 0950 0 0\n",
      "62 1601 0 0\n",
      "62 1811 0 0\n",
      "62 2289 0 0\n",
      "62 2664 0 0\n",
      "62 2714 0 0\n",
      "62 3075 0 0\n",
      "62 3156 0 0\n",
      "63 0950 0 0\n",
      "63 1601 0 0\n",
      "63 1795 0 0\n",
      "63 1811 0 0\n",
      "63 2266 0 0\n",
      "63 2289 0 0\n",
      "63 2557 0 0\n",
      "63 2664 0 0\n",
      "63 2714 0 0\n",
      "63 2973 0 0\n",
      "63 3075 0 0\n",
      "63 3156 0 0\n",
      "64 2651 0 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Read the stop words file\n",
    "with open (\"./datasets/qrels.text\",\"r\") as file:\n",
    "    qr_file = \"\".join(file.readlines())\n",
    "    print(qr_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The first column is the query_id and the second column is the document_id. We can safely ignore the 3rd and 4th columns.\n",
    "You can use the implemented function 'read_qrels'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.evaluation import read_qrels\n",
    "\n",
    "#### Function check\n",
    "qrels = read_qrels()\n",
    "\n",
    "assert len(qrels) == 52, \"There should be 52 queries with relevance judgements\"\n",
    "assert sum(len(j) for j in qrels.values()) == 796, \"There should be a total of 796 Relevance Judgements\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Note:** For a given query `query_id`, you can assume that documents *not* in `qrels[query_id]` are not relevant to `query_id`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.2 Precision\n",
    "Implement the `precision@k` metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: What articles exist which deal with TSS (Time Sharing System), an\n",
      "operating system for IBM computers?\n",
      "precision@10 = 0.2\n"
     ]
    }
   ],
   "source": [
    "# ToDo:\n",
    "# Implement the following function 'precision_k'!\n",
    "from modules.evaluation import precision_k\n",
    "\n",
    "#### Function check\n",
    "qid = queries[0][0]\n",
    "qtext = queries[0][1]\n",
    "print(f'query:{qtext}')\n",
    "results = bm25_search(qtext, dh2)\n",
    "precision = precision_k(results, qrels[qid], 10)\n",
    "print(f'precision@10 = {precision}')\n",
    "assert precision is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.3 Recall \n",
    "Implement the `recall@k` metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: SETL, Very High Level Languages\n",
      "recall@10 = 0.3157894736842105\n"
     ]
    }
   ],
   "source": [
    "# ToDo:\n",
    "# Implement the following function\n",
    "from modules.evaluation import recall_k\n",
    "\n",
    "#### Function check\n",
    "qid = queries[10][0]\n",
    "qtext = queries[10][1]\n",
    "print(f'query:{qtext}')\n",
    "results = bm25_search(qtext, dh2)\n",
    "recall = recall_k(results, qrels[qid], 10)\n",
    "print(f'recall@10 = {recall}')\n",
    "assert recall is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.4 Mean Average Precision\n",
    "Implement the `map` metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: computational complexity, intractability, class-complete reductions,\n",
      "algorithms and efficiency\n",
      "MAP = 0.17240404110559454\n"
     ]
    }
   ],
   "source": [
    "# ToDo:\n",
    "# Implement the following function\n",
    "from modules.evaluation import average_precision\n",
    "\n",
    "#### Function check\n",
    "qid = queries[20][0]\n",
    "qtext = queries[20][1]\n",
    "print(f'query:{qtext}')\n",
    "results = bm25_search(qtext, dh2)\n",
    "mean_ap = average_precision(results, qrels[qid])\n",
    "print(f'MAP = {mean_ap}')\n",
    "assert mean_ap is not None"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c2c5ccf7e153c3826497608a13106df1b3c5c34cecf259281315c8b06c776444"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
